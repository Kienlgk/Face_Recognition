arguments: align/align_dataset_mtcnn.py lfw lfw_test --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.85
--------------------
git hash: b'09a78657f8f0467c43f200234e5090aacb57f520'
--------------------
b'diff --git a/lib/src/align/align_dataset_mtcnn.py b/lib/src/align/align_dataset_mtcnn.py\nindex adebc97..1329f3b 100644\n--- a/lib/src/align/align_dataset_mtcnn.py\n+++ b/lib/src/align/align_dataset_mtcnn.py\n@@ -34,7 +34,7 @@ import numpy as np\n import facenet\n import align.detect_face\n import random\n-from time import sleep\n+from time import sleep, time\n \n def main(args):\n     sleep(random.random())\n@@ -86,6 +86,7 @@ def main(args):\n                         errorMessage = \'{}: {}\'.format(image_path, e)\n                         print(errorMessage)\n                     else:\n+                        start = time()\n                         if img.ndim<2:\n                             print(\'Unable to align "%s"\' % image_path)\n                             text_file.write(\'%s\\n\' % (output_filename))\n@@ -132,7 +133,8 @@ def main(args):\n                         else:\n                             print(\'Unable to align "%s"\' % image_path)\n                             text_file.write(\'%s\\n\' % (output_filename))\n-                            \n+                        end = time()\n+                        print(\'Cost: {} s for 1 image.\'.format(end-start))\n     print(\'Total number of images: %d\' % nrof_images_total)\n     print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n             \ndiff --git a/lib/src/create_face_embeddings.py b/lib/src/create_face_embeddings.py\nindex 1b1ac59..ddeccdf 100644\n--- a/lib/src/create_face_embeddings.py\n+++ b/lib/src/create_face_embeddings.py\n@@ -51,12 +51,20 @@ def main(args):\n             # ashish = [\'datasets/kar_Vin_aligned/Ashish/\' + f for f in os.listdir(\'datasets/kar_Vin_aligned/Ashish\')]\n             # saurabh = [\'datasets/kar_Vin_aligned/Saurabh/\' + f for f in os.listdir(\'datasets/kar_Vin_aligned/Saurabh\')]\n             # hari = [\'datasets/kar_Vin_aligned/Hari/\' + f for f in os.listdir(\'datasets/kar_Vin_aligned/Hari\')]\n-            vinayak =  [os.path.join(\'data_160x160\', \'4\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'4\'))]\n-            karthik =  [os.path.join(\'data_160x160\', \'6\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'6\'))]\n-            ashish = [os.path.join(\'data_160x160\', \'15\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'15\'))]\n-            saurabh = [os.path.join(\'data_160x160\', \'16\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'16\'))]\n-            hari = [os.path.join(\'data_160x160\', \'20\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'20\'))]\n-            paths = vinayak+karthik+ashish+saurabh+hari\n+            # vinayak =  [os.path.join(\'data_160x160\', \'4\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'4\'))]\n+            # karthik =  [os.path.join(\'data_160x160\', \'6\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'6\'))]\n+            # ashish = [os.path.join(\'data_160x160\', \'15\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'15\'))]\n+            # saurabh = [os.path.join(\'data_160x160\', \'16\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'16\'))]\n+            # hari = [os.path.join(\'data_160x160\', \'20\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'20\'))]\n+            data_dir = \'data_160x160\'\n+            folders = os.listdir(data_dir)\n+            paths = []\n+            for folder in folders[:-2]:\n+                paths += [os.path.join(data_dir, folder, f) for f in os.listdir(os.path.join(data_dir, folder))]\n+            print("removed:")\n+            for folder in folders[-2:]:\n+                print(folder)\n+            # paths = vinayak+karthik+ashish+saurabh+hari\n             #np.save("images.npy",paths)\n             # Load the model\n             facenet.load_model(args.model)\n@@ -78,10 +86,10 @@ def main(args):\n                 feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n                 feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n                 extracted_dict[filename] =  feature_vector\n-                if(i%100 == 0):\n-                    print("completed",i," images")\n+                # if(i%1 == 0):\n+                print("completed",i," images")\n \n-            with open(\'extracted_dict.pickle\',\'wb\') as f:\n+            with open(\'embedding_dict.pickle\',\'wb\') as f:\n                 pickle.dump(extracted_dict,f)\n \n             \n@@ -91,7 +99,7 @@ def parse_arguments(argv):\n \n     parser.add_argument(\'--lfw_batch_size\', type=int,\n         help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'--model\', type=str,default=\'E:/workspace/deep_learning/Face_Recognition/lib/weights/20180408-102900\', \n+    parser.add_argument(\'--model\', type=str,default=\'~/workspace/hdd/Kien/Face_Recognition/lib/src/ckpt/20180402-114759\', \n         help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n     parser.add_argument(\'--image_size\', type=int,\n         help=\'Image size (height, width) in pixels.\', default=160)\ndiff --git a/lib/src/recognize.py b/lib/src/recognize.py\nindex f008135..4764d87 100644\n--- a/lib/src/recognize.py\n+++ b/lib/src/recognize.py\n@@ -7,87 +7,135 @@ import sys\n import random\n from six import iteritems\n import numpy as np\n-import retrieve\n+# import retrieve\n from align import detect_face\n import tensorflow as tf\n import pickle\n import time\n import argparse\n+import cv2\n+from sklearn import metrics\n+from scipy.optimize import brentq\n+from scipy import interpolate\n+from scipy import misc\n+\n+from facenet import load_data\n+from facenet import load_img\n+from facenet import load_model\n+from facenet import to_rgb\n+from facenet import get_model_filenames\n \n def main(args):\n-    ckpt = args.ckpt\n-    embedding_dir = args.embedding_dir\n+    ckpt = os.path.expanduser(args.ckpt)\n+    embedding_dir = os.path.expanduser(args.embedding_dir)\n     gpu_memory_fraction = args.gpu_memory_fraction\n     graph_fr = tf.Graph()\n     # Config memory for GTX 1080 with 8GB VRAM\n     gpu_memory_fraction = 0.4\n-    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+    # config = tf.ConfigProto()\n+    # config.gpu_options.allow_growth=True\n+    # sess = tf.Session(config=config)\n+    # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+    gpu_options = tf.GPUOptions(allow_growth=True)\n     sess_fr = tf.Session(graph=graph_fr, config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+    meta_graph_file, ckpt_file = get_model_filenames(ckpt)\n     with graph_fr.as_default():\n-        saverf = tf.train.import_meta_graph(os.path.join(ckpt, \'model-20180402-114759.meta\'))\n-        saverf.restore(sess_fr, os.path.join(ckpt, \'model-20180402-114759.ckpt-275\'))\n+        saverf = tf.train.import_meta_graph(os.path.join(ckpt, meta_graph_file))\n+        saverf.restore(sess_fr, os.path.join(ckpt, ckpt_file))\n         pnet, rnet, onet = detect_face.create_mtcnn(sess_fr, None)\n \n     with open(embedding_dir, \'rb\') as f:\n         embedding_dict = pickle.load(f)\n     \n     def face_det():\n-        retrieve.recognize_face(sess_fr, pnet, rnet, onet, embedding_dict)\n+        recognize_face(sess_fr, pnet, rnet, onet, feature_array=embedding_dict, args=args)\n+\n \n     face_det()\n \n \n-def recognize_face(sess,pnet, rnet, onet,feature_array):\n+def recognize_face(sess, pnet, rnet, onet, feature_array, args):\n     # Get input and output tensors\n     images_placeholder = sess.graph.get_tensor_by_name("input:0")\n     images_placeholder = tf.image.resize_images(images_placeholder,(160,160))\n     embeddings = sess.graph.get_tensor_by_name("embeddings:0")\n     phase_train_placeholder = sess.graph.get_tensor_by_name("phase_train:0")\n-\n     image_size = args.image_size\n     embedding_size = embeddings.get_shape()[1]\n-\n-    cap = cv2.VideoCapture(-1)\n-\n-    while(True):\n-        ret, frame = cap.read()\n-        gray = cv2.cvtColor(frame, 0)\n-        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-            cap.release()\n-            cv2.destroyAllWindows()\n-            break\n+    imgdir = os.path.expanduser(args.imgdir)\n+    if os.path.isfile(imgdir):\n+        gray = cv2.imread(imgdir, cv2.IMREAD_GRAYSCALE)\n         if(gray.size > 0):\n             print(gray.size)\n-            response, faces,bboxs = align_face(gray,pnet, rnet, onet)\n+            start = time.time()\n+            response, faces,bboxs = align_face(gray,pnet, rnet, onet, args)\n+            align = time.time()\n             print(response)\n             if (response == True):\n-                    for i, image in enumerate(faces):\n-                            bb = bboxs[i]\n-                            images = load_img(image, False, False, image_size)\n-                            feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n-                            feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n-                            result, accuracy = identify_person(feature_vector, feature_array,8)\n-                            print(result.split("/")[2])\n-                            print(accuracy)\n-\n-                            if accuracy < 0.9:\n-                                cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n-                                W = int(bb[2]-bb[0])//2\n-                                H = int(bb[3]-bb[1])//2\n-                                cv2.putText(gray,"Hello "+result.split("/")[2],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n-                            else:\n-                                cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n-                                W = int(bb[2]-bb[0])//2\n-                                H = int(bb[3]-bb[1])//2\n-                                cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n-                            del feature_vector\n+                for i, image in enumerate(faces):\n+                    bb = bboxs[i]\n+                    images = load_img(image, False, False, image_size)\n+                    feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+                    feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n+                    result, accuracy = identify_person(feature_vector, feature_array, 8)\n+                    print(result.split("/")[1])\n+                    print(accuracy)\n+\n+                    # if accuracy < 0.9:\n+                    #     cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                    #     W = int(bb[2]-bb[0])//2\n+                    #     H = int(bb[3]-bb[1])//2\n+                    #     cv2.putText(gray,"Hello "+result.split("/")[1],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    # else:\n+                    #     cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                    #     W = int(bb[2]-bb[0])//2\n+                    #     H = int(bb[3]-bb[1])//2\n+                    #     cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    del feature_vector\n+            stop = time.time()\n+            print(\'Cost {} s for embedding 1 image\'.format(stop - start))\n+            print(\'Cost {} s for align 1 image\'.format(align-start))\n+            # cv2.imshow(\'img\',gray)\n+            cv2.waitKey(0)\n+        return\n+    image_dirs = os.listdir(imgdir)\n+    for image_dir in image_dirs:\n+        imgdir =  os.path.join(imgdir, image_dir)\n+\n+        gray = cv2.imread(imgdir, cv2.IMREAD_GRAYSCALE)\n+        if(gray.size > 0):\n+            print(gray.size)\n+            response, faces,bboxs = align_face(gray,pnet, rnet, onet, args)\n+            print(response)\n+            if (response == True):\n+                for i, image in enumerate(faces):\n+                    bb = bboxs[i]\n+                    images = load_img(image, False, False, image_size)\n+                    feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+                    feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n+                    result, accuracy = identify_person(feature_vector, feature_array, 8)\n+                    print(result.split("/")[1])\n+                    print(accuracy)\n+\n+                    if accuracy < 0.9:\n+                        cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                        W = int(bb[2]-bb[0])//2\n+                        H = int(bb[3]-bb[1])//2\n+                        cv2.putText(gray,"Hello "+result.split("/")[1],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    else:\n+                        cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                        W = int(bb[2]-bb[0])//2\n+                        H = int(bb[3]-bb[1])//2\n+                        cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    del feature_vector\n \n             cv2.imshow(\'img\',gray)\n+            cv2.waitKey(0)\n         else:\n             continue\n \n \n-def align_face(img,pnet, rnet, onet):\n+def align_face(img, pnet, rnet, onet, args):\n \n     minsize = 20 # minimum size of face\n     threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n@@ -142,7 +190,7 @@ def align_face(img,pnet, rnet, onet):\n             bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n             cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n             scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n-            misc.imsave("cropped.png", scaled)\n+            # misc.imsave("cropped.png", scaled)\n             faces.append(scaled)\n             bboxes.append(bb)\n             print("leaving align face")\n@@ -151,19 +199,25 @@ def align_face(img,pnet, rnet, onet):\n \n def identify_person(image_vector, feature_array, k=9):\n     top_k_ind = np.argsort([np.linalg.norm(image_vector-pred_row) \\\n-                        for ith_row, pred_row in enumerate(feature_array.values())])[:k]\n-    result = feature_array.keys()[top_k_ind[0]]\n-    acc = np.linalg.norm(image_vector-feature_array.values()[top_k_ind[0]])\n+                        for ith_row, pred_row in enumerate(list(feature_array.values()))])[:k]\n+    result = list(feature_array.keys())[top_k_ind[0]]\n+    acc = np.linalg.norm(image_vector-list(feature_array.values())[top_k_ind[0]])\n     return result, acc\n \n \n if __name__ == "__main__":\n     handler = argparse.ArgumentParser()\n-    handler.add_argument("--imgdir", help="Testing images directory")\n-    handler.add_argument("--embedding_dir", help="Directory of the embedding file")\n-    handler.add_argument("--ckpt", help="Model\'s weights .ckpt file")\n+    handler.add_argument("--imgdir", default="", help="Testing images directory")\n+    handler.add_argument("--embedding_dir", default="~/workspace/hdd/Kien/Face_Recognition/lib/src/embedding_dict.pickle", \n+                        help="Directory of the embedding file")\n+    handler.add_argument("--ckpt", default="~/workspace/hdd/Kien/Face_Recognition/lib/src/ckpt/20180402-114759",\n+                        help="Model\'s weights .ckpt file")\n     handler.add_argument("--gpu_memory_fraction", default=0.85)\n-\n+    handler.add_argument(\'--image_size\', type=int, help=\'Image size (height, width) in pixels.\', default=160)\n+    handler.add_argument(\'--detect_multiple_faces\', type=bool,\n+                        help=\'Detect and align multiple faces per image.\', default=True)\n+    handler.add_argument(\'--margin\', type=int,\n+                        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n     # handler.add_argument()\n     FLAGS = handler.parse_args()\n     main(FLAGS)\n\\ No newline at end of file'