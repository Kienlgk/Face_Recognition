arguments: lib/src/train_tripletloss.py --pretrained_model /home/na/workspace/deep_learning/Kien/Face_Recognition/lib/weights/20180408-102900/model-20180408-102900
--------------------
git hash: b'09a78657f8f0467c43f200234e5090aacb57f520'
--------------------
b'diff --git a/.gitignore b/.gitignore\nold mode 100644\nnew mode 100755\ndiff --git a/README.md b/README.md\nold mode 100644\nnew mode 100755\ndiff --git a/__init__.py b/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/__init__.pyc b/__init__.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/LICENSE.md b/lib/LICENSE.md\nold mode 100644\nnew mode 100755\ndiff --git a/lib/__init__.py b/lib/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/__init__.pyc b/lib/__init__.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/__init__.py b/lib/contributed/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/batch_represent.py b/lib/contributed/batch_represent.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/cluster.py b/lib/contributed/cluster.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/clustering.py b/lib/contributed/clustering.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/export_embeddings.py b/lib/contributed/export_embeddings.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/face.py b/lib/contributed/face.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/predict.py b/lib/contributed/predict.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/contributed/real_time_face_recognition.py b/lib/contributed/real_time_face_recognition.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/data/images/Anthony_Hopkins_0001.jpg b/lib/data/images/Anthony_Hopkins_0001.jpg\nold mode 100644\nnew mode 100755\ndiff --git a/lib/data/images/Anthony_Hopkins_0002.jpg b/lib/data/images/Anthony_Hopkins_0002.jpg\nold mode 100644\nnew mode 100755\ndiff --git a/lib/data/learning_rate_retrain_tripletloss.txt b/lib/data/learning_rate_retrain_tripletloss.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/data/learning_rate_schedule_classifier_casia.txt b/lib/data/learning_rate_schedule_classifier_casia.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/data/learning_rate_schedule_classifier_msceleb.txt b/lib/data/learning_rate_schedule_classifier_msceleb.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/data/pairs.txt b/lib/data/pairs.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/__init__.py b/lib/src/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/__init__.pyc b/lib/src/__init__.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/__init__.py b/lib/src/align/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/__init__.pyc b/lib/src/align/__init__.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/align_dataset.py b/lib/src/align/align_dataset.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/align_dataset_mtcnn.py b/lib/src/align/align_dataset_mtcnn.py\nold mode 100644\nnew mode 100755\nindex adebc97..1329f3b\n--- a/lib/src/align/align_dataset_mtcnn.py\n+++ b/lib/src/align/align_dataset_mtcnn.py\n@@ -34,7 +34,7 @@ import numpy as np\n import facenet\n import align.detect_face\n import random\n-from time import sleep\n+from time import sleep, time\n \n def main(args):\n     sleep(random.random())\n@@ -86,6 +86,7 @@ def main(args):\n                         errorMessage = \'{}: {}\'.format(image_path, e)\n                         print(errorMessage)\n                     else:\n+                        start = time()\n                         if img.ndim<2:\n                             print(\'Unable to align "%s"\' % image_path)\n                             text_file.write(\'%s\\n\' % (output_filename))\n@@ -132,7 +133,8 @@ def main(args):\n                         else:\n                             print(\'Unable to align "%s"\' % image_path)\n                             text_file.write(\'%s\\n\' % (output_filename))\n-                            \n+                        end = time()\n+                        print(\'Cost: {} s for 1 image.\'.format(end-start))\n     print(\'Total number of images: %d\' % nrof_images_total)\n     print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n             \ndiff --git a/lib/src/align/align_dlib.py b/lib/src/align/align_dlib.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/det1.npy b/lib/src/align/det1.npy\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/det2.npy b/lib/src/align/det2.npy\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/det3.npy b/lib/src/align/det3.npy\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/align/detect_face.py b/lib/src/align/detect_face.py\nold mode 100644\nnew mode 100755\nindex 29e13be..da17730\n--- a/lib/src/align/detect_face.py\n+++ b/lib/src/align/detect_face.py\n@@ -82,7 +82,7 @@ class Network(object):\n         session: The current TensorFlow session\n         ignore_missing: If true, serialized weights for missing layers are ignored.\n         \'\'\'\n-        data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n+        data_dict = np.load(data_path, encoding=\'latin1\', allow_pickle=True).item() #pylint: disable=no-member\n \n         for op_name in data_dict:\n             with tf.variable_scope(op_name, reuse=True):\ndiff --git a/lib/src/align/detect_face.pyc b/lib/src/align/detect_face.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/calculate_filtering_metrics.py b/lib/src/calculate_filtering_metrics.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/classifier.py b/lib/src/classifier.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/commands.txt b/lib/src/commands.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/compare.py b/lib/src/compare.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/create_face_embeddings.py b/lib/src/create_face_embeddings.py\nold mode 100644\nnew mode 100755\nindex 1b1ac59..b85e4f0\n--- a/lib/src/create_face_embeddings.py\n+++ b/lib/src/create_face_embeddings.py\n@@ -51,12 +51,20 @@ def main(args):\n             # ashish = [\'datasets/kar_Vin_aligned/Ashish/\' + f for f in os.listdir(\'datasets/kar_Vin_aligned/Ashish\')]\n             # saurabh = [\'datasets/kar_Vin_aligned/Saurabh/\' + f for f in os.listdir(\'datasets/kar_Vin_aligned/Saurabh\')]\n             # hari = [\'datasets/kar_Vin_aligned/Hari/\' + f for f in os.listdir(\'datasets/kar_Vin_aligned/Hari\')]\n-            vinayak =  [os.path.join(\'data_160x160\', \'4\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'4\'))]\n-            karthik =  [os.path.join(\'data_160x160\', \'6\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'6\'))]\n-            ashish = [os.path.join(\'data_160x160\', \'15\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'15\'))]\n-            saurabh = [os.path.join(\'data_160x160\', \'16\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'16\'))]\n-            hari = [os.path.join(\'data_160x160\', \'20\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'20\'))]\n-            paths = vinayak+karthik+ashish+saurabh+hari\n+            # vinayak =  [os.path.join(\'data_160x160\', \'4\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'4\'))]\n+            # karthik =  [os.path.join(\'data_160x160\', \'6\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'6\'))]\n+            # ashish = [os.path.join(\'data_160x160\', \'15\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'15\'))]\n+            # saurabh = [os.path.join(\'data_160x160\', \'16\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'16\'))]\n+            # hari = [os.path.join(\'data_160x160\', \'20\', f) for f in os.listdir(os.path.join(\'data_160x160\', \'20\'))]\n+            data_dir = os.path.expanduser(os.path.join(\'~\', \'workspace\', \'hdd\', \'data\', \'celebrity_7650_folders\'))\n+            folders = os.listdir(data_dir)\n+            paths = []\n+            for folder in folders[:-2]:\n+                paths += [os.path.join(data_dir, folder, f) for f in os.listdir(os.path.join(data_dir, folder))]\n+            print("removed:")\n+            for folder in folders[-2:]:\n+                print(folder)\n+            # paths = vinayak+karthik+ashish+saurabh+hari\n             #np.save("images.npy",paths)\n             # Load the model\n             facenet.load_model(args.model)\n@@ -78,10 +86,10 @@ def main(args):\n                 feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n                 feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n                 extracted_dict[filename] =  feature_vector\n-                if(i%100 == 0):\n+                if(i%1000 == 0):\n                     print("completed",i," images")\n-\n-            with open(\'extracted_dict.pickle\',\'wb\') as f:\n+            print("Done.")\n+            with open(\'embedding_dict.pickle\',\'wb\') as f:\n                 pickle.dump(extracted_dict,f)\n \n             \n@@ -91,7 +99,7 @@ def parse_arguments(argv):\n \n     parser.add_argument(\'--lfw_batch_size\', type=int,\n         help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'--model\', type=str,default=\'E:/workspace/deep_learning/Face_Recognition/lib/weights/20180408-102900\', \n+    parser.add_argument(\'--model\', type=str,default=\'~/workspace/hdd/Kien/Face_Recognition/lib/src/ckpt/20180402-114759\', \n         help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n     parser.add_argument(\'--image_size\', type=int,\n         help=\'Image size (height, width) in pixels.\', default=160)\ndiff --git a/lib/src/decode_msceleb_dataset.py b/lib/src/decode_msceleb_dataset.py\nold mode 100644\nnew mode 100755\nindex 4556bfa..7948c97\n--- a/lib/src/decode_msceleb_dataset.py\n+++ b/lib/src/decode_msceleb_dataset.py\n@@ -33,6 +33,8 @@ import base64\n import sys\n import os\n import cv2\n+import io\n+from PIL import Image\n import argparse\n import facenet\n \n@@ -61,10 +63,11 @@ def main(args):\n             fields = line.split(\'\\t\')\n             class_dir = fields[0]\n             img_name = fields[1] + \'-\' + fields[4] + \'.\' + args.output_format\n-            img_string = fields[5]\n+            img_string = fields[6]\n             img_dec_string = base64.b64decode(img_string)\n             img_data = np.fromstring(img_dec_string, dtype=np.uint8)\n             img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\n+            # img = cv2.imdecode(img_data, cv2.IMREAD_ANYCOLOR) #pylint: disable=maybe-no-member\n             if args.size:\n                 img = misc.imresize(img, (args.size, args.size), interp=\'bilinear\')\n             full_class_dir = os.path.join(output_dir, class_dir)\ndiff --git a/lib/src/download_and_extract_model.py b/lib/src/download_and_extract_model.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/download_vgg_face_dataset.py b/lib/src/download_vgg_face_dataset.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/face_embeddings.pickle b/lib/src/face_embeddings.pickle\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/face_embeddings_old.pickle b/lib/src/face_embeddings_old.pickle\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/facenet.py b/lib/src/facenet.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/facenet.pyc b/lib/src/facenet.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/freeze_graph.py b/lib/src/freeze_graph.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generated_pairs.txt b/lib/src/generated_pairs.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/__init__.py b/lib/src/generative/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/calculate_attribute_vectors.py b/lib/src/generative/calculate_attribute_vectors.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/calculate_dataset_normalization.py b/lib/src/generative/calculate_dataset_normalization.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/models/__init__.py b/lib/src/generative/models/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/models/dfc_vae.py b/lib/src/generative/models/dfc_vae.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/models/dfc_vae_large.py b/lib/src/generative/models/dfc_vae_large.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/models/dfc_vae_resnet.py b/lib/src/generative/models/dfc_vae_resnet.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/models/vae_base.py b/lib/src/generative/models/vae_base.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/modify_attribute.py b/lib/src/generative/modify_attribute.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/generative/train_vae.py b/lib/src/generative/train_vae.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/lfw.py b/lib/src/lfw.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/lfw.pyc b/lib/src/lfw.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/log_val_far.txt b/lib/src/log_val_far.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/models/__init__.py b/lib/src/models/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/models/inception_resnet_v1.py b/lib/src/models/inception_resnet_v1.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/models/inception_resnet_v2.py b/lib/src/models/inception_resnet_v2.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/models/squeezenet.py b/lib/src/models/squeezenet.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/pairs.txt b/lib/src/pairs.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/recognize.py b/lib/src/recognize.py\nold mode 100644\nnew mode 100755\nindex f008135..88caa6b\n--- a/lib/src/recognize.py\n+++ b/lib/src/recognize.py\n@@ -7,37 +7,178 @@ import sys\n import random\n from six import iteritems\n import numpy as np\n-import retrieve\n+from datetime import datetime\n+from imutils.video import FPS\n+import threading \n+# from threading import Lock\n+\n+BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n+ROOT_DIR = os.path.dirname(BASE_DIR)\n+sys.path.append(BASE_DIR)\n+sys.path.append(ROOT_DIR)\n+\n+# import retrieve\n from align import detect_face\n import tensorflow as tf\n import pickle\n import time\n import argparse\n+import cv2\n+from sklearn import metrics\n+from scipy.optimize import brentq\n+from scipy import interpolate\n+from scipy import misc\n+\n+from facenet import load_data\n+from facenet import load_img\n+from facenet import load_model\n+from facenet import to_rgb\n+from facenet import get_model_filenames\n+\n+import queue\n+\n+# bufferless VideoCapture\n+class VideoCapture:\n+  def __init__(self, name):\n+    self.cap = cv2.VideoCapture(name)\n+    self.q = queue.Queue()\n+    t = threading.Thread(target=self._reader)\n+    t.daemon = True\n+    t.start()\n+\n+  # read frames as soon as they are available, keeping only most recent one\n+  def _reader(self):\n+    while True:\n+      ret, frame = self.cap.read()\n+      if not ret:\n+        break\n+      if not self.q.empty():\n+        try:\n+          self.q.get_nowait()   # discard previous (unprocessed) frame\n+        except queue.Empty:\n+          pass\n+      self.q.put(frame)\n+\n+  def read(self):\n+    return self.q.get()\n \n def main(args):\n-    ckpt = args.ckpt\n-    embedding_dir = args.embedding_dir\n+    ckpt = os.path.expanduser(args.ckpt)\n+    embedding_dir = os.path.expanduser(args.embedding_dir)\n+    is_stream = args.stream\n     gpu_memory_fraction = args.gpu_memory_fraction\n     graph_fr = tf.Graph()\n     # Config memory for GTX 1080 with 8GB VRAM\n     gpu_memory_fraction = 0.4\n-    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+    # config = tf.ConfigProto()\n+    # config.gpu_options.allow_growth=True\n+    # sess = tf.Session(config=config)\n+    # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n+    gpu_options = tf.GPUOptions(allow_growth=True)\n     sess_fr = tf.Session(graph=graph_fr, config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+    meta_graph_file, ckpt_file = get_model_filenames(ckpt)\n     with graph_fr.as_default():\n-        saverf = tf.train.import_meta_graph(os.path.join(ckpt, \'model-20180402-114759.meta\'))\n-        saverf.restore(sess_fr, os.path.join(ckpt, \'model-20180402-114759.ckpt-275\'))\n+        saverf = tf.train.import_meta_graph(os.path.join(ckpt, meta_graph_file))\n+        saverf.restore(sess_fr, os.path.join(ckpt, ckpt_file))\n         pnet, rnet, onet = detect_face.create_mtcnn(sess_fr, None)\n \n     with open(embedding_dir, \'rb\') as f:\n         embedding_dict = pickle.load(f)\n     \n-    def face_det():\n-        retrieve.recognize_face(sess_fr, pnet, rnet, onet, embedding_dict)\n+    def face_det(is_stream):\n+        if is_stream:\n+            recognize_face_stream(sess_fr, pnet, rnet, onet, feature_array=embedding_dict, args=args)\n+        else:\n+            recognize_face(sess_fr, pnet, rnet, onet, feature_array=embedding_dict, args=args)\n+\n+    face_det(is_stream)\n+\n+def recognize_face(sess, pnet, rnet, onet, feature_array, args):\n+    # Get input and output tensors\n+    images_placeholder = sess.graph.get_tensor_by_name("input:0")\n+    images_placeholder = tf.image.resize_images(images_placeholder,(160,160))\n+    embeddings = sess.graph.get_tensor_by_name("embeddings:0")\n+    phase_train_placeholder = sess.graph.get_tensor_by_name("phase_train:0")\n+    image_size = args.image_size\n+    embedding_size = embeddings.get_shape()[1]\n+    imgdir = os.path.expanduser(args.imgdir)\n+    if os.path.isfile(imgdir):\n+        gray = cv2.imread(imgdir, cv2.IMREAD_GRAYSCALE)\n+        if(gray.size > 0):\n+            print(gray.size)\n+            start = time.time()\n+            response, faces,bboxs = align_face(gray,pnet, rnet, onet, args)\n+            align = time.time()\n+            print(response)\n+            if (response == True):\n+                for i, image in enumerate(faces):\n+                    bb = bboxs[i]\n+                    images = load_img(image, False, False, image_size)\n+                    feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+                    feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n+                    result, accuracy = identify_person(feature_vector, feature_array, 8)\n+                    print(result.split("/")[1])\n+                    print(accuracy)\n+\n+                    # if accuracy < 0.9:\n+                    #     cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                    #     W = int(bb[2]-bb[0])//2\n+                    #     H = int(bb[3]-bb[1])//2\n+                    #     cv2.putText(gray,"Hello "+result.split("/")[1],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    # else:\n+                    #     cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                    #     W = int(bb[2]-bb[0])//2\n+                    #     H = int(bb[3]-bb[1])//2\n+                    #     cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    del feature_vector\n+            stop = time.time()\n+            print(\'Cost {} s for embedding 1 image\'.format(stop - start))\n+            print(\'Cost {} s for align 1 image\'.format(align-start))\n+            # cv2.imshow(\'img\',gray)\n+            cv2.waitKey(0)\n+        return\n+    image_dirs = os.listdir(imgdir)\n+    for image_dir in image_dirs:\n+        image_dir =  os.path.join(imgdir, image_dir)\n+        gray = cv2.imread(image_dir, cv2.IMREAD_GRAYSCALE)\n+        if(gray.size > 0):\n+            start = time.time()\n+            print(gray.size)\n+            response, faces,bboxs = align_face(gray,pnet, rnet, onet, args)\n+            print(response)\n+            if (response == True):\n+                for i, image in enumerate(faces):\n+                    bb = bboxs[i]\n+                    images = load_img(image, False, False, image_size)\n+                    feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+                    feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n+                    result, accuracy = identify_person(feature_vector, feature_array, 8)\n+                    print(result.split("/")[1])\n+                    print(accuracy)\n \n-    face_det()\n+                    if accuracy < 0.9:\n+                        cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                        W = int(bb[2]-bb[0])//2\n+                        H = int(bb[3]-bb[1])//2\n+                        cv2.putText(gray,"Hello "+result.split("/")[1],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    else:\n+                        cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                        W = int(bb[2]-bb[0])//2\n+                        H = int(bb[3]-bb[1])//2\n+                        cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    del feature_vector\n+            stop = time.time()\n+            print(\'Cost {} s for embedding 1 image\'.format(stop - start))\n+            # cv2.imshow(\'img\',gray)\n+            # cv2.waitKey(0)\n+        else:\n+            continue\n \n+# lo = Lock()\n+latest_frame = None\n+last_ret = None\n \n-def recognize_face(sess,pnet, rnet, onet,feature_array):\n+def recognize_face_stream(sess, pnet, rnet, onet, feature_array, args):\n     # Get input and output tensors\n     images_placeholder = sess.graph.get_tensor_by_name("input:0")\n     images_placeholder = tf.image.resize_images(images_placeholder,(160,160))\n@@ -46,50 +187,99 @@ def recognize_face(sess,pnet, rnet, onet,feature_array):\n \n     image_size = args.image_size\n     embedding_size = embeddings.get_shape()[1]\n+    \n+    # cap = cv2.VideoCapture("http://192.168.1.12:81/videostream.cgi?user=admin&pwd=123456789?action=stream?dummy=param.mjpg")\n+    # cap = cv2.VideoCapture("http://192.168.1.12:81/videostream.cgi?user=admin&pwd=123456789")\n+    cap = VideoCapture("rtsp://admin:KtxBk$2019@222.253.145.118:55431/h264/ch1/main/av_stream")\n+    # cap = VideoCapture("rtsp://admin:KtxBk$2019@222.253.145.118:55432/h264/ch1/main/av_stream")\n+    # cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n+    \n \n-    cap = cv2.VideoCapture(-1)\n+    # def rtsp_cam_buffer(vcap):\n+    #     global latest_frame, last_ret\n+    #     while True:\n+    #         last_ret, latest_frame = vcap.read()\n \n+    # t1 = threading.Thread(target=rtsp_cam_buffer,args=(cap, ),name="rtsp_read_thread")\n+    # t1.daemon=True\n+    # t1.start()\n+    # fps = FPS().start()\n+        \n+    threshold = 0.9\n     while(True):\n-        ret, frame = cap.read()\n-        gray = cv2.cvtColor(frame, 0)\n+        # if (last_ret is not None) and (latest_frame is not None):\n+        #     frame = latest_frame.copy()\n+        # else:\n+        #     print("Error: failed to capture image")\n+        #     break\n+        frame = cap.read()\n+        # ret, frame = cap.read()\n+        cap_time = datetime.now()\n+        # gray = cv2.cvtColor(frame, 0)\n+        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n+        # gray = gray[384:, 216:]\n+        # gray = cv2.resize(gray, (960, 540))\n+        gray = cv2.resize(gray, (960, 540))\n+        frame = cv2.resize(frame, (960, 540))\n+        # gray = cv2.resize(gray, (768, 432))\n         if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n             cap.release()\n             cv2.destroyAllWindows()\n             break\n-        if(gray.size > 0):\n-            print(gray.size)\n-            response, faces,bboxs = align_face(gray,pnet, rnet, onet)\n+        if (gray.size > 0):\n+            start = time.time()\n+            response, faces,bboxs = align_face(gray,pnet, rnet, onet, args)\n+            align_face_time = time.time()\n             print(response)\n             if (response == True):\n-                    for i, image in enumerate(faces):\n-                            bb = bboxs[i]\n-                            images = load_img(image, False, False, image_size)\n-                            feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n-                            feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n-                            result, accuracy = identify_person(feature_vector, feature_array,8)\n-                            print(result.split("/")[2])\n-                            print(accuracy)\n-\n-                            if accuracy < 0.9:\n-                                cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n-                                W = int(bb[2]-bb[0])//2\n-                                H = int(bb[3]-bb[1])//2\n-                                cv2.putText(gray,"Hello "+result.split("/")[2],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n-                            else:\n-                                cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n-                                W = int(bb[2]-bb[0])//2\n-                                H = int(bb[3]-bb[1])//2\n-                                cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n-                            del feature_vector\n-\n-            cv2.imshow(\'img\',gray)\n+                # cv2.imwrite(os.path.expanduser(os.path.join(\'~\',\'workspace\',\'hdd\',\'Kien\', \'data\', \'cam\', \'1\', cap_time.strftime("%m_%d_%Y_%H_%M__S")+".png")), gray)\n+                cv2.imwrite(os.path.expanduser(os.path.join(\'~\',\'workspace\',\'hdd\',\'Kien\', \'data\', \'cam\', \'tuong_collect\', str(cap_time)+".png")), frame)\n+                # for i, image in enumerate(faces):\n+                #     bb = bboxs[i]\n+                #     images = load_img(image, False, False, image_size)\n+                #     feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+                #     # detect face only\n+                #     cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                #     W = int(bb[2]-bb[0])//2\n+                #     H = int(bb[3]-bb[1])//2\n+                #     cv2.putText(gray,"Face",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                #     # Calculate face embedding and compare with trained ones\n+                #     feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n+                #     result, accuracy = identify_person(feature_vector, feature_array, 8)\n+                #     print(result.split("/")[1])\n+                #     print(accuracy)\n+                #     if accuracy < threshold:\n+                #         cv2.rectangle(frame,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                #         W = int(bb[2]-bb[0])//2\n+                #         H = int(bb[3]-bb[1])//2\n+                #         cv2.putText(frame,"Hello "+result.split("/")[1],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                #     else:\n+                #         cv2.rectangle(frame,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                #         W = int(bb[2]-bb[0])//2\n+                #         H = int(bb[3]-bb[1])//2\n+                #         cv2.putText(frame,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                #     del feature_vector\n+                # # cv2.imwrite(os.path.expanduser(os.path.join(\'~\',\'workspace\',\'hdd\',\'Kien\', \'data\', \'cam\', \'vom\', str(cap_time)+".png")), gray)\n+                # # cv2.imwrite(os.path.expanduser(os.path.join(\'~\',\'workspace\',\'hdd\',\'Kien\', \'data\', \'cam\', \'tuong\', str(cap_time)+".png")), frame)\n+                # cv2.imwrite(os.path.expanduser(os.path.join(\'~\',\'workspace\',\'hdd\',\'Kien\', \'data\', \'cam\', \'tuong_collect\', str(cap_time)+".png")), frame)\n+\n+\n+            stop = time.time()\n+            print(\'Cost {} s for aligning 1 image\'.format(align_face_time - start))\n+            print(\'Cost {} s for embedding 1 image\'.format(stop - start))\n+            cv2.imshow(\'img\',frame)\n+            # cv2.waitKey(0)\n+            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n+                break\n         else:\n             continue\n+    cap.release()\n+    pass\n \n \n-def align_face(img,pnet, rnet, onet):\n+def align_face(img, pnet, rnet, onet, args):\n \n-    minsize = 20 # minimum size of face\n+    minsize = 30 # minimum size of face\n     threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n     factor = 0.709 # scale factor\n \n@@ -142,7 +332,7 @@ def align_face(img,pnet, rnet, onet):\n             bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n             cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n             scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n-            misc.imsave("cropped.png", scaled)\n+            # misc.imsave("cropped.png", scaled)\n             faces.append(scaled)\n             bboxes.append(bb)\n             print("leaving align face")\n@@ -150,20 +340,35 @@ def align_face(img,pnet, rnet, onet):\n \n \n def identify_person(image_vector, feature_array, k=9):\n+    # features = np.asarray(list(feature_array.values()))\n+    # noEmbedding = features.shape[0]\n+    # image_vector = np.tile(image_vector, (noEmbedding, 1))\n+    # top_k_ind = np.argsort(np.linalg.norm(image_vector-features, axis=1))\n     top_k_ind = np.argsort([np.linalg.norm(image_vector-pred_row) \\\n-                        for ith_row, pred_row in enumerate(feature_array.values())])[:k]\n-    result = feature_array.keys()[top_k_ind[0]]\n-    acc = np.linalg.norm(image_vector-feature_array.values()[top_k_ind[0]])\n+                        for pred_row in (list(feature_array.values()))])[:k]\n+                        # for ith_row, pred_row in enumerate(list(feature_array.values()))])[:k]\n+\n+    result = list(feature_array.keys())[top_k_ind[0]]\n+    acc = np.linalg.norm(image_vector-list(feature_array.values())[top_k_ind[0]])\n     return result, acc\n \n \n-if __name__ == "__main__":\n+def arguments_parser(argv):\n     handler = argparse.ArgumentParser()\n-    handler.add_argument("--imgdir", help="Testing images directory")\n-    handler.add_argument("--embedding_dir", help="Directory of the embedding file")\n-    handler.add_argument("--ckpt", help="Model\'s weights .ckpt file")\n+    handler.add_argument("--imgdir", default="", help="Testing images directory")\n+    handler.add_argument("--embedding_dir", default="~/workspace/hdd/Kien/Face_Recognition/lib/src/embedding_dict.pickle", \n+                        help="Directory of the embedding file")\n+    handler.add_argument("--ckpt", default="~/workspace/hdd/Kien/Face_Recognition/lib/src/ckpt/20180402-114759",\n+                        help="Model\'s weights .ckpt file")\n     handler.add_argument("--gpu_memory_fraction", default=0.85)\n+    handler.add_argument(\'--image_size\', type=int, help=\'Image size (height, width) in pixels.\', default=160)\n+    handler.add_argument(\'--detect_multiple_faces\', type=bool,\n+                        help=\'Detect and align multiple faces per image.\', default=True)\n+    handler.add_argument(\'--margin\', type=int,\n+                        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n+    handler.add_argument("--stream", default=False, action=\'store_true\')\n+    return handler.parse_args()\n \n-    # handler.add_argument()\n-    FLAGS = handler.parse_args()\n-    main(FLAGS)\n\\ No newline at end of file\n+\n+if __name__ == "__main__":\n+    main(arguments_parser(sys.argv[1:]))\n\\ No newline at end of file\ndiff --git a/lib/src/resize_recursive.py b/lib/src/resize_recursive.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/retrieve.py b/lib/src/retrieve.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/retrieve.pyc b/lib/src/retrieve.pyc\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/train_softmax.py b/lib/src/train_softmax.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/train_tripletloss.py b/lib/src/train_tripletloss.py\nold mode 100644\nnew mode 100755\nindex 9f7e866..54dc7dc\n--- a/lib/src/train_tripletloss.py\n+++ b/lib/src/train_tripletloss.py\n@@ -31,6 +31,7 @@ from datetime import datetime\n import os.path\n import time\n import sys\n+\n import tensorflow as tf\n import numpy as np\n import importlib\n@@ -39,6 +40,9 @@ import argparse\n import facenet\n import lfw\n \n+BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n+ROOT_DIR = os.path.dirname(BASE_DIR)\n+\n from tensorflow.python.ops import data_flow_ops\n \n from six.moves import xrange\n@@ -48,10 +52,10 @@ def main(args):\n     network = importlib.import_module(args.model_def)\n \n     subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n+    log_dir = os.path.join(os.path.join(ROOT_DIR, os.path.expanduser(args.logs_base_dir)), subdir)\n     if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n         os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n+    model_dir = os.path.join(os.path.join(ROOT_DIR, os.path.expanduser(args.models_base_dir)), subdir)\n     if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n         os.makedirs(model_dir)\n \n@@ -157,7 +161,8 @@ def main(args):\n         summary_op = tf.summary.merge_all()\n \n         # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        gpu_options = tf.GPUOptions(allow_growth=True)\n         sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n \n         # Initialize variables\n@@ -417,16 +422,16 @@ def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n     \n     parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n+        help=\'Directory where to write event logs.\', default=\'logs\')\n     parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n+        help=\'Directory where to write trained models and checkpoints.\', default=\'weights\')\n     parser.add_argument(\'--gpu_memory_fraction\', type=float,\n         help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=0.85)\n     parser.add_argument(\'--pretrained_model\', type=str,\n         help=\'Load a pretrained model before training starts.\')\n     parser.add_argument(\'--data_dir\', type=str,\n         help=\'Path to the data directory containing aligned face patches. Multiple directories are separated with colon.\',\n-        default=\'/home/na/workspace/hdd/data/celebrity\')\n+        default=\'/home/na/workspace/deep_learning/Downloads/celebrity\')\n         # default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n     parser.add_argument(\'--model_def\', type=str,\n         help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n@@ -445,7 +450,7 @@ def parse_arguments(argv):\n     parser.add_argument(\'--alpha\', type=float,\n         help=\'Positive to negative triplet distance margin.\', default=0.2)\n     parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=512)\n+        help=\'Dimensionality of the embedding.\', default=128)\n     parser.add_argument(\'--random_crop\', \n         help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n          \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n@@ -470,7 +475,7 @@ def parse_arguments(argv):\n         help=\'Random seed.\', default=666)\n     parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n         help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\',\n-        default=\'/home/na/workspace/hdd/Kien/Face_Recognition/lib/data/learning_rate_schedule_classifier_msceleb.txt\')\n+        default=\'/home/na/workspace/hdd/Kien/Face_Recognition/lib/data/learning_rate_msceleb_tripletloss.txt\')\n         # default=\'data/learning_rate_schedule.txt\')\n \n     # Parameters for validation on LFW\ndiff --git a/lib/src/validate_on_lfw.py b/lib/src/validate_on_lfw.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/view_model.py b/lib/src/view_model.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/src/write_lfw_format.py b/lib/src/write_lfw_format.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/test/batch_norm_test.py b/lib/test/batch_norm_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/test/center_loss_test.py b/lib/test/center_loss_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/test/decov_loss_test.py b/lib/test/decov_loss_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/test/restore_test.py b/lib/test/restore_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/test/train_test.py b/lib/test/train_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/test/triplet_loss_test.py b/lib/test/triplet_loss_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/__init__.py b/lib/tmp/__init__.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/align_dataset.m b/lib/tmp/align_dataset.m\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/cacd2000_split_identities.py b/lib/tmp/cacd2000_split_identities.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/dataset_read_speed.py b/lib/tmp/dataset_read_speed.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/deepdream.py b/lib/tmp/deepdream.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/detect_face_v1.m b/lib/tmp/detect_face_v1.m\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/detect_face_v2.m b/lib/tmp/detect_face_v2.m\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/funnel_dataset.py b/lib/tmp/funnel_dataset.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/invariance_test.txt b/lib/tmp/invariance_test.txt\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/mnist_center_loss.py b/lib/tmp/mnist_center_loss.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/mnist_noise_labels.py b/lib/tmp/mnist_noise_labels.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/mtcnn.py b/lib/tmp/mtcnn.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/mtcnn_test.py b/lib/tmp/mtcnn_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/mtcnn_test_pnet_dbg.py b/lib/tmp/mtcnn_test_pnet_dbg.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/network.py b/lib/tmp/network.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/nn2.py b/lib/tmp/nn2.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/nn3.py b/lib/tmp/nn3.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/nn4.py b/lib/tmp/nn4.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/nn4_small2_v1.py b/lib/tmp/nn4_small2_v1.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/pilatus800.jpg b/lib/tmp/pilatus800.jpg\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/random_test.py b/lib/tmp/random_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/rename_casia_directories.py b/lib/tmp/rename_casia_directories.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/seed_test.py b/lib/tmp/seed_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/select_triplets_test.py b/lib/tmp/select_triplets_test.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/test1.py b/lib/tmp/test1.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/test_align.py b/lib/tmp/test_align.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/test_invariance_on_lfw.py b/lib/tmp/test_invariance_on_lfw.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/vggface16.py b/lib/tmp/vggface16.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/vggverydeep19.py b/lib/tmp/vggverydeep19.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/visualize.py b/lib/tmp/visualize.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/visualize_vgg_model.py b/lib/tmp/visualize_vgg_model.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/tmp/visualize_vggface.py b/lib/tmp/visualize_vggface.py\nold mode 100644\nnew mode 100755\ndiff --git a/lib/util/plot_learning_curves.m b/lib/util/plot_learning_curves.m\nold mode 100644\nnew mode 100755\ndiff --git a/requirements.txt b/requirements.txt\nold mode 100644\nnew mode 100755\ndiff --git a/server/rest-server.py b/server/rest-server.py\nold mode 100644\nnew mode 100755\ndiff --git a/server/static/images/1.jpg b/server/static/images/1.jpg\nold mode 100644\nnew mode 100755\ndiff --git a/server/static/images/2.png b/server/static/images/2.png\nold mode 100644\nnew mode 100755\ndiff --git a/server/static/images/face_recog.png b/server/static/images/face_recog.png\nold mode 100644\nnew mode 100755\ndiff --git a/server/static/images/vinayak.jpeg b/server/static/images/vinayak.jpeg\nold mode 100644\nnew mode 100755\ndiff --git a/server/templates/main.html b/server/templates/main.html\nold mode 100644\nnew mode 100755'